from __future__ import annotations

import tilelang
import tilelang.language as T
import argparse
import torch
import torch.distributed as dist
import torch.multiprocessing
from tilelang.distributed import init_dist
from tilelang.distributed import perf_fn
from reduce_scatter import reduce_scatter_2d_op, create_reduce_scater_2d_ctx

tilelang.disable_cache()


@tilelang.jit
def gemm_kernel(M,
                N,
                K,
                local_rank,
                num_local_rank,
                block_M,
                block_N,
                block_K,
                threads,
                persistent=False,
                dtype="float16",
                accum_dtype="float"):

    M_per_rank = T.ceildiv(M, num_local_rank)
    GROUP_SIZE_M = 8
    
    def swizzle_2d(tile_id, num_pid_m, num_pid_n):
        num_pid_in_group = GROUP_SIZE_M * num_pid_n
        group_id = tile_id // num_pid_in_group
        first_pid_m = group_id * GROUP_SIZE_M
        group_size_m = T.min(num_pid_m - first_pid_m, GROUP_SIZE_M)
        pid_m = first_pid_m + (tile_id % group_size_m)
        pid_n = (tile_id % num_pid_in_group) // group_size_m
        return pid_m, pid_n

    @T.prim_func
    def main(
            A: T.Tensor((M, K // num_local_rank), dtype),
            B: T.Tensor((K // num_local_rank, N), dtype),
            scatter_signal_buf: T.Tensor((num_local_rank), "uint32"),
            counter_signal_buf: T.Tensor((num_local_rank), "uint32"),
            C: T.Tensor((M, N), dtype),
    ):
        with T.Kernel(T.ceildiv(M, block_M) * T.ceildiv(N, block_N), threads=threads) as (bid):
            A_shared = T.alloc_shared((block_M, block_K), dtype)
            B_shared = T.alloc_shared((block_K, block_N), dtype)
            C_shared = T.alloc_shared((block_M, block_N), dtype)
            C_local = T.alloc_fragment((block_M, block_N), accum_dtype)
            val = T.alloc_local((1,), "uint32")

            num_pid_m = T.ceildiv(M, block_M)
            num_pid_n = T.ceildiv(N, block_N)
            
            pid_m_, pid_n = swizzle_2d(bid, num_pid_m, num_pid_n)
            pid_m_offset = (local_rank + 1) * M_per_rank // block_M
            pid_m = (pid_m_ + pid_m_offset) % num_pid_m
        
            tid = T.get_thread_binding(0)
            T.clear(C_local)
            for k in T.Pipelined(T.ceildiv(K // num_local_rank, block_K), num_stages=3):
                T.copy(A[pid_m * block_M, k * block_K], A_shared)
                T.copy(B[k * block_K, pid_n * block_N], B_shared)
                T.gemm(A_shared, B_shared, C_local)
            T.copy(C_local, C_shared)
            T.copy(C_shared, C[pid_m * block_M, pid_n * block_N])

            # inc barrier
            segment_start = pid_m * block_M // M_per_rank
            segment_end = (T.min((pid_m + 1) * block_M, M) - 1) // M_per_rank
            segment = segment_start + tid
            if segment <= segment_end:
                m_start = M_per_rank * segment
                m_end = M_per_rank * (segment + 1) - 1
                tiled_m_start = m_start // block_M
                tiled_m_end = m_end // block_M
                tiled_m_size = tiled_m_end - tiled_m_start + 1
                val[0] = T.atom_add(counter_signal_buf[segment], 1, scope="gpu", sem="release")
                if T.Cast("int32", val[0]) == num_pid_n * tiled_m_size - 1:
                    T.st(scatter_signal_buf[segment], 1, scope="gpu", sem="release")

    return main


def gemm_rs_op(A,
               B,
               C,
               output,
               ctx,
               gemm_kernel,
               gemm_stream,
               rs_stream,
               local_rank,
               print_source=False):

    current_stream = torch.cuda.current_stream()
    rs_stream.wait_stream(gemm_stream)

    gemm_kernel(
        A,
        B,
        ctx.scatter_signal_bufs[local_rank],
        ctx.counter_bufs[local_rank],
        C,
        stream=gemm_stream.cuda_stream)

    if print_source and local_rank == 1:
        print(gemm_kernel.get_kernel_source())

    with torch.cuda.stream(rs_stream):
        # don't allocate memory on other stream: error-prune
        output = reduce_scatter_2d_op(C, ctx, output)
    gemm_stream.wait_stream(rs_stream)
    current_stream.wait_stream(rs_stream)

    return output


def torch_gemm_rs(
    pg: torch.distributed.ProcessGroup,
    input: torch.Tensor,  # [M, local_k]
    weight: torch.Tensor,  # [local_K, N]
    bias: torch.Tensor | None,
    num_local_ranks: int,
):
    M, local_K = input.shape
    N = weight.shape[1]
    output = torch.matmul(input, weight)
    if bias:
        output = output + bias
    rs_output = torch.empty((M // num_local_ranks, N), dtype=output.dtype, device=input.device)
    torch.distributed.reduce_scatter_tensor(rs_output, output, group=pg)
    return rs_output


def main(local_rank: int, num_local_ranks: int, args: argparse.Namespace):
    dtype = torch.float16
    M = args.M if args else 8192
    N = args.N if args else 8192
    K = args.K if args else 8192
    persistent = args.persistent
    M_per_rank = M // num_local_ranks
    K_per_rank = K // num_local_ranks

    BLOCK_M = 128
    BLOCK_N = 256
    BLOCK_K = 64
    threads = 256

    rank, num_ranks, group = init_dist(local_rank, num_local_ranks)
    assert rank == local_rank and num_ranks == num_local_ranks, "only support single node for now"
    allocator = tilelang.get_allocator(
        size=2**30,
        device="cuda",
        is_distributed=True,
        local_rank=local_rank,
        num_local_ranks=num_local_ranks,
        group=group)
    gemm_func = gemm_kernel(M, N, K, local_rank, num_local_ranks, BLOCK_M, BLOCK_N, BLOCK_K,
                            threads, persistent)
    gemm_func.initialize(allocator=allocator)

    A = tilelang.tensor((M, K_per_rank), dtype, allocator=allocator).normal_() / 10
    B = tilelang.tensor((K_per_rank, N), dtype, allocator=allocator).normal_() / 10
    C = tilelang.tensor((M, N), dtype, allocator=allocator)
    output = tilelang.tensor((M_per_rank, N), dtype, allocator=allocator)
    gemm_stream = torch.cuda.Stream()
    rs_stream = torch.cuda.Stream(priority=-1)
    ctx = create_reduce_scater_2d_ctx(
        M,
        N,
        local_rank,
        num_local_ranks,
        num_local_ranks,
        dtype,
        allocator,
        overlap_with_gemm=True,
        num_reduction_sms=15)

    dist.barrier()

    tilelang_out = gemm_rs_op(
        A, B, C, output, ctx, gemm_func, gemm_stream, rs_stream, local_rank, print_source=True)
    torch_out = torch_gemm_rs(group, A, B, None, num_local_ranks)

    atol = 1e-2
    rtol = 1e-2
    if torch.allclose(torch_out, tilelang_out, atol=atol, rtol=rtol):
        print(f"rank {local_rank} check passed.✅")
    else:
        print(f"rank {local_rank} check failed.❌")
        print(f"torch_out: {torch_out}, tilelang_out: {tilelang_out}")

    _, tl_t = perf_fn(
        lambda: gemm_rs_op(A, B, C, output, ctx, gemm_func, gemm_stream, rs_stream, local_rank),
        warmup=5,
        rep=5)

    print(
        f"rank {local_rank} tilelang gemm_rs time: {tl_t:.2f} ms, TFLOPS: {2*M*N*K/1e9/(tl_t)/num_local_ranks:.2f}"
    )

    dist.destroy_process_group()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--num-processes', type=int, default=2, help='Number of processes to spawn (default: 2)')
    parser.add_argument('--M', type=int, default=8192, help='M dimension')
    parser.add_argument('--N', type=int, default=8192, help='N dimension')
    parser.add_argument('--K', type=int, default=29568, help='K dimension')
    parser.add_argument('--persistent', action='store_true', help='Use persistent kernel')
    args = parser.parse_args()
    num_processes = args.num_processes

    torch.multiprocessing.spawn(main, args=(num_processes, args), nprocs=num_processes)
