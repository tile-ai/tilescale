import os
import tilelang
import tilelang.language as T
from tilelang.carver.arch import driver
import torch
import torch.distributed as dist
import torch.multiprocessing
import argparse
from tilelang.distributed.utils import init_dist

tilelang.disable_cache()
os.environ['NCCL_DEBUG'] = 'WARN'  # silence NCCL log


@tilelang.jit(
    out_idx=-1, pass_configs={
        "tl.disable_warp_specialized": True,
        "tl.disable_tma_lower": True
    })
def get_test_barrierall_sys_kernel(num_ranks: int, blocks: int, threads: int):

    @T.prim_func
    def main(
            A: T.Tensor([threads], "int32"),  # type: ignore
            barrier: T.Tensor([num_ranks], "int32"),  # type: ignore
            B: T.Tensor([blocks, threads], "int32"),  # type: ignore
    ):
        with T.Kernel(blocks, threads=threads) as bid:
            tid = T.get_thread_binding()
            rank = T.alloc_local([1], "int32")
            rank[0] = T.get_rank()
            val = T.alloc_local([1], "int32")
            val[0] = 1
            T.atomic_add(A[tid], val[0])

            T.barrier_all_blocks_sys(barrier)

            if tid < 32:
                T.pull_warp(
                    src=T.address_of(A),
                    dst=T.address_of(B[bid, 0]),
                    size=threads,
                    src_pe=rank[0] ^ 1,
                    unroll_factor=4)

    return main


def main(local_rank: int, num_ranks: int, args: argparse.Namespace):
    blocks, threads = args.blocks, args.threads

    _, _, group = init_dist(local_rank, num_ranks)
    allocator = tilelang.get_allocator(
        size=2**20,
        device="cuda",
        is_distributed=True,
        local_rank=local_rank,
        num_local_ranks=num_ranks,
        group=group)
    kernel = get_test_barrierall_sys_kernel(num_ranks, blocks, threads)
    kernel.initialize(allocator=allocator)

    A = tilelang.tensor([threads], torch.int32, allocator=allocator).zero_()
    barrier = tilelang.tensor([num_ranks], torch.int32, allocator=allocator)
    torch.cuda.synchronize()
    dist.barrier(group)
    output = kernel(A, barrier)
    torch.cuda.synchronize()
    dist.barrier(group)
    if torch.all(output == blocks):
        print(f"rank {local_rank} check passed.✅")
    else:
        print(f"rank {local_rank} check failed.❌")
        print(f"output: {output}")

    dist.destroy_process_group()


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--num-processes', type=int, default=2, help='Number of processes to spawn (default: 2)')
    parser.add_argument('--blocks', type=int, default=64, help='Number of blocks (default: 64)')
    parser.add_argument('--threads', type=int, default=128, help='Number of threads (default: 128)')
    parser.add_argument(
        '--print-source', action='store_true', help='Print the source code of the kernel')
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    num_processes = args.num_processes
    assert args.blocks <= driver.get_num_sms(
    ), f'Launched {args.blocks} blocks, which is larger than the number of SM ({driver.get_num_sms()}) on the current device and may cause deadlock!'

    torch.multiprocessing.spawn(main, args=(num_processes, args), nprocs=num_processes)
